{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9899e7a0d771483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import gc\n",
    "from sklearn.metrics import precision_recall_curve, precision_score as sklearn_precision_score, \\\n",
    "    recall_score as sklearn_recall_score, roc_curve\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "# Enable numerical checking to identify where NaNs occur\n",
    "tf.debugging.enable_check_numerics()\n",
    "\n",
    "# TPU Setup\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "print(\"TPU initialized successfully!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Constants with memory and numerical stability in mind\n",
    "IMAGE_SIZE = (224, 224)\n",
    "TPU_BATCH_SIZE = 64  # Reduced batch size for stability\n",
    "GLOBAL_BATCH_SIZE = min(TPU_BATCH_SIZE, TPU_BATCH_SIZE * strategy.num_replicas_in_sync)\n",
    "\n",
    "# Data constants\n",
    "train_samples = 11200\n",
    "train_tfrecords = [f'/content/drive/MyDrive/DermaLyticsAI/small dataset/train.tfrecord']\n",
    "\n",
    "val_samples = 4800\n",
    "val_tfrecords = [f'/content/drive/MyDrive/DermaLyticsAI/small dataset/validation.tfrecord']\n",
    "\n",
    "total_test_samples = 4000\n",
    "test_tfrecords = [f'/content/drive/MyDrive/DermaLyticsAI/small dataset/test.tfrecord']\n",
    "\n",
    "checkpoint_path = f'/content/drive/MyDrive/DermaLyticsAI/small dataset/modelCheckpoint.weights.h5'\n",
    "log_dir = f'/content/drive/MyDrive/DermaLyticsAI/small dataset/logs'\n",
    "modelSave_path = f'/content/drive/MyDrive/DermaLyticsAI/small dataset/model/derma_model.h5'\n",
    "modelKerasSave_path = f'/content/drive/MyDrive/DermaLyticsAI/small dataset/model/derma_model_keras.keras'"
   ],
   "id": "2c2aee1a9f08b03b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "id": "54ea9066abe21d0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_function(proto, image_size, is_training=True):\n",
    "    \"\"\"Parse a single example from TFRecord with additional error handling.\"\"\"\n",
    "    # Define the feature description\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([1], tf.float32),\n",
    "        \"metadata\": tf.io.FixedLenFeature([11], tf.float32),\n",
    "        \"mask\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "\n",
    "    # Parse with error handling\n",
    "    try:\n",
    "        example = tf.io.parse_single_example(proto, feature_description)\n",
    "\n",
    "        # Decode and process the image with error clipping\n",
    "        image = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "        image = tf.image.resize(image, image_size)\n",
    "        image = tf.clip_by_value(tf.cast(image, tf.float32) / 255.0, 0.0, 1.0)  # Ensure proper range\n",
    "\n",
    "        # Decode and process the mask with error clipping\n",
    "        mask = tf.io.decode_jpeg(example[\"mask\"], channels=3)\n",
    "        mask = tf.image.resize(mask, image_size)\n",
    "        mask = tf.clip_by_value(tf.cast(mask, tf.float32) / 255.0, 0.0, 1.0)  # Ensure proper range\n",
    "\n",
    "        if is_training:\n",
    "            # Generate random transformation parameters\n",
    "            seed = tf.random.uniform([2], minval=0, maxval=100, dtype=tf.int32)\n",
    "\n",
    "            # Apply identical flips using the same seed\n",
    "            image = tf.image.stateless_random_flip_left_right(image, seed)\n",
    "            mask = tf.image.stateless_random_flip_left_right(mask, seed)\n",
    "            image = tf.image.stateless_random_flip_up_down(image, seed)\n",
    "            mask = tf.image.stateless_random_flip_up_down(mask, seed)\n",
    "\n",
    "            # Random rotation\n",
    "            flipNumber = tf.random.uniform([], 0, 4, dtype=tf.int32)\n",
    "            image = tf.image.rot90(image, k=flipNumber)\n",
    "            mask = tf.image.rot90(mask, k=flipNumber)\n",
    "\n",
    "        # Extract and normalize metadata\n",
    "        metadata = example[\"metadata\"]\n",
    "        # Clip metadata to reasonable ranges to prevent extreme values\n",
    "        metadata = tf.clip_by_value(metadata, -100.0, 100.0)\n",
    "\n",
    "        # Make sure label is valid binary\n",
    "        label = tf.clip_by_value(example[\"label\"], 0.0, 1.0)\n",
    "\n",
    "        # Return using a dictionary structure\n",
    "        return ({\"image_input\": image, \"mask_input\": mask, \"metadata_input\": metadata}, label)\n",
    "\n",
    "    except tf.errors.InvalidArgumentError as e:\n",
    "        # On parse error, return a safe default\n",
    "        print(f\"Error parsing example: {e}\")\n",
    "        # Return zeros with correct shapes\n",
    "        default_image = tf.zeros((*image_size, 3), dtype=tf.float32)\n",
    "        default_mask = tf.zeros((*image_size, 3), dtype=tf.float32)\n",
    "        default_metadata = tf.zeros((11,), dtype=tf.float32)\n",
    "        default_label = tf.constant([[0.0]], dtype=tf.float32)\n",
    "\n",
    "        return ({\"image_input\": default_image, \"mask_input\": default_mask, \"metadata_input\": default_metadata},\n",
    "                default_label)"
   ],
   "id": "1246a7975829b29d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_tfrecord_dataset(tfrecord_paths, batch_size, image_size, epochs=1, is_training=True):\n",
    "    \"\"\"Load TFRecord dataset with enhanced stability and error handling.\"\"\"\n",
    "    # Use the parse function\n",
    "    parse_fn = lambda x: parse_function(x, image_size, is_training)\n",
    "\n",
    "    # Create the dataset with error handling\n",
    "    try:\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            tfrecord_paths,\n",
    "            num_parallel_reads=tf.data.AUTOTUNE,\n",
    "            buffer_size=8 * 1024 * 1024  # 8MB buffer\n",
    "        )\n",
    "\n",
    "        # Handle corrupted records by skipping them\n",
    "        dataset = dataset.map(\n",
    "            parse_fn,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        if is_training:\n",
    "            dataset = dataset.shuffle(1000)  # Reduced buffer size\n",
    "\n",
    "        # Batch first, then repeat to ensure complete batches\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "        # Apply repeat after batching\n",
    "        if is_training:\n",
    "            dataset = dataset.repeat(epochs)\n",
    "        else:\n",
    "            # Just enough repetitions for validation\n",
    "            dataset = dataset.repeat(2)\n",
    "\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        # Calculate steps more conservatively\n",
    "        if is_training:\n",
    "            steps = (train_samples // batch_size) - 2  # Buffer to avoid StopIteration\n",
    "        else:\n",
    "            if tfrecord_paths[0] == val_tfrecords[0]:\n",
    "                steps = (val_samples // batch_size) - 2\n",
    "            else:\n",
    "                steps = (total_test_samples // batch_size) - 2\n",
    "\n",
    "        # Ensure we have at least 1 step\n",
    "        steps = max(1, steps)\n",
    "\n",
    "        return dataset, steps\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        raise"
   ],
   "id": "4a07884e97254f60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_class_distribution(dataset, steps):\n",
    "    \"\"\"Calculate the ratio of positive samples in the dataset.\"\"\"\n",
    "    positive_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for i, (_, labels) in enumerate(dataset):\n",
    "        if i >= steps:\n",
    "            break\n",
    "        positive_count += tf.reduce_sum(labels).numpy()\n",
    "        total_count += labels.shape[0]\n",
    "\n",
    "    return positive_count / total_count if total_count > 0 else 0"
   ],
   "id": "c62014176e5e678e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_model(strategy, image_size=(224, 224)):\n",
    "    \"\"\"Build model with enhanced numerical stability.\"\"\"\n",
    "    with strategy.scope():\n",
    "        # Set up kernel initializers for stability\n",
    "        kernel_init = tf.keras.initializers.GlorotNormal(seed=42)\n",
    "\n",
    "        # Define the inputs\n",
    "        image_input = tf.keras.Input(shape=(*image_size, 3), name=\"image_input\")\n",
    "        mask_input = tf.keras.Input(shape=(*image_size, 3), name=\"mask_input\")\n",
    "        metadata_input = tf.keras.Input(shape=(11,), name=\"metadata_input\")\n",
    "\n",
    "        # Attention mechanism with stabilized initialization\n",
    "        attention = tf.keras.layers.Conv2D(\n",
    "            1, (1, 1),\n",
    "            activation='sigmoid',\n",
    "            kernel_initializer=kernel_init\n",
    "        )(mask_input)\n",
    "        modulated_image = tf.keras.layers.Multiply()([image_input, attention])\n",
    "\n",
    "        # Load EfficientNetV2B3 with weight initialization for stability\n",
    "        base_model = tf.keras.applications.EfficientNetV2B3(\n",
    "            include_top=False,\n",
    "            input_tensor=modulated_image,\n",
    "            weights=None,  # Start with random weights for TPU compatibility\n",
    "            classes=2\n",
    "        )\n",
    "\n",
    "        # Freeze initial layers to stabilize training\n",
    "        for layer in base_model.layers[:50]:  # Freeze early layers\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Make later layers trainable\n",
    "        for layer in base_model.layers[50:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # Add attention mechanism with stable initialization\n",
    "        attention = tf.keras.layers.Conv2D(\n",
    "            base_model.output.shape[-1],\n",
    "            (1, 1),\n",
    "            activation='sigmoid',\n",
    "            kernel_initializer=kernel_init\n",
    "        )(base_model.output)\n",
    "\n",
    "        weighted_features = tf.keras.layers.Multiply()([base_model.output, attention])\n",
    "\n",
    "        # Process image features with batch normalization for stability\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(weighted_features)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)  # Add dropout for regularization\n",
    "\n",
    "        # Process mask with a small CNN to extract features\n",
    "        mask_features = tf.keras.layers.Conv2D(\n",
    "            16, (3, 3),\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "            kernel_initializer=kernel_init\n",
    "        )(mask_input)\n",
    "        mask_features = tf.keras.layers.BatchNormalization()(mask_features)\n",
    "        mask_features = tf.keras.layers.MaxPooling2D()(mask_features)\n",
    "\n",
    "        mask_features = tf.keras.layers.Conv2D(\n",
    "            32, (3, 3),\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "            kernel_initializer=kernel_init\n",
    "        )(mask_features)\n",
    "        mask_features = tf.keras.layers.BatchNormalization()(mask_features)\n",
    "        mask_features = tf.keras.layers.GlobalAveragePooling2D()(mask_features)\n",
    "        mask_features = tf.keras.layers.BatchNormalization()(mask_features)\n",
    "\n",
    "        # Split metadata with more stable processing\n",
    "        demographic_features = tf.keras.layers.Lambda(lambda x: x[:, :3])(metadata_input)\n",
    "        location_features = tf.keras.layers.Lambda(lambda x: x[:, 3:])(metadata_input)\n",
    "\n",
    "        # Process location features\n",
    "        location_encoded = tf.keras.layers.Dense(\n",
    "            16,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=kernel_init,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "        )(location_features)\n",
    "        location_encoded = tf.keras.layers.BatchNormalization()(location_encoded)\n",
    "        location_encoded = tf.keras.layers.Dropout(0.1)(location_encoded)\n",
    "\n",
    "        # Process demographic features\n",
    "        demographic_encoded = tf.keras.layers.Dense(\n",
    "            16,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=kernel_init,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "        )(demographic_features)\n",
    "        demographic_encoded = tf.keras.layers.BatchNormalization()(demographic_encoded)\n",
    "        demographic_encoded = tf.keras.layers.Dropout(0.1)(demographic_encoded)\n",
    "\n",
    "        # Combine metadata features\n",
    "        metadata_features = tf.keras.layers.Concatenate()([location_encoded, demographic_encoded])\n",
    "        metadata_features = tf.keras.layers.BatchNormalization()(metadata_features)\n",
    "\n",
    "        # Combine all features with BatchNormalization\n",
    "        combined = tf.keras.layers.Concatenate()([x, metadata_features, mask_features])\n",
    "        combined = tf.keras.layers.BatchNormalization()(combined)\n",
    "\n",
    "        # First dense block with residual connection and more regularization\n",
    "        block1 = tf.keras.layers.Dense(\n",
    "            256,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=kernel_init,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "        )(combined)\n",
    "        block1 = tf.keras.layers.BatchNormalization()(block1)\n",
    "        block1 = tf.keras.layers.Dropout(0.3)(block1)\n",
    "\n",
    "        # Residual connection with the same architecture\n",
    "        block1_res = tf.keras.layers.Dense(\n",
    "            256,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=kernel_init,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "        )(block1)\n",
    "        block1_res = tf.keras.layers.BatchNormalization()(block1_res)\n",
    "        block1_res = tf.keras.layers.Dropout(0.3)(block1_res)\n",
    "\n",
    "        # Add residual connection for improved gradient flow\n",
    "        block1 = tf.keras.layers.Add()([block1, block1_res])\n",
    "        block1 = tf.keras.layers.BatchNormalization()(block1)\n",
    "\n",
    "        # Second dense block\n",
    "        block2 = tf.keras.layers.Dense(\n",
    "            128,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=kernel_init,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "        )(block1)\n",
    "        block2 = tf.keras.layers.BatchNormalization()(block2)\n",
    "        block2 = tf.keras.layers.Dropout(0.2)(block2)\n",
    "\n",
    "        # Ensure numerical stability with float32 precision\n",
    "        combined = tf.keras.layers.Lambda(lambda x: tf.cast(x, tf.float32))(block2)\n",
    "\n",
    "        # Final classification layer with stable initialization\n",
    "        output = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=\"sigmoid\",\n",
    "            dtype='float32',  # Explicitly use float32 for stability\n",
    "            kernel_initializer=kernel_init,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "        )(combined)\n",
    "\n",
    "        model = tf.keras.Model(inputs=[image_input, metadata_input, mask_input], outputs=output)\n",
    "\n",
    "        return model"
   ],
   "id": "c5edf7aa2f78bf3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define callbacks for training with additional stability monitoring\n",
    "early_stopping_auc = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_AUC\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode=\"max\"\n",
    ")\n",
    "early_stopping_loss = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=7,\n",
    "    restore_best_weights=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "terminate_on_nan = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    update_freq='epoch'\n",
    ")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_AUC',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Load datasets with stability improvements\n",
    "train_dataset, train_steps = load_tfrecord_dataset(\n",
    "    train_tfrecords,\n",
    "    batch_size=GLOBAL_BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    epochs=40,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "val_dataset, val_steps = load_tfrecord_dataset(\n",
    "    val_tfrecords,\n",
    "    batch_size=GLOBAL_BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    epochs=40,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(f\"Steps per epoch: {train_steps}, Validation steps: {val_steps}\")\n",
    "\n",
    "# Build and compile model with TPU strategy\n",
    "with strategy.scope():\n",
    "    # Build model with stabilized architecture\n",
    "    model = build_model(strategy, IMAGE_SIZE)\n",
    "\n",
    "    # More conservative learning rate schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-4,  # Lower initial learning rate\n",
    "        decay_steps=train_steps * 5,\n",
    "        decay_rate=0.95,\n",
    "        staircase=True\n",
    "    )\n",
    "\n",
    "    # Optimizer with enhanced numerical stability\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=lr_schedule,\n",
    "        clipnorm=1.0,        # Stricter gradient clipping\n",
    "        epsilon=1e-7         # Increased epsilon for numerical stability\n",
    "    )\n",
    "\n",
    "    # Compile model with loss scaling for mixed precision\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.5),\n",
    "            tf.keras.metrics.AUC(name=\"AUC\", curve='ROC'),\n",
    "            tf.keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n",
    "            tf.keras.metrics.Recall(name=\"recall\", thresholds=0.5),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ],
   "id": "7f376cc8d92f5a45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Start with a stability check - short training to identify issues early\n",
    "print(\"PHASE 1: Initial stability check (2 epochs)\")\n",
    "try:\n",
    "    initial_history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=2,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=[terminate_on_nan],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Check if training was stable\n",
    "    if np.isnan(initial_history.history['loss'][-1]):\n",
    "        print(\"WARNING: NaN detected in initial training. Adjusting parameters...\")\n",
    "\n",
    "        # Reset model with even more conservative settings\n",
    "        with strategy.scope():\n",
    "            # Clear memory\n",
    "            K.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "            # Rebuild with more conservative settings\n",
    "            model = build_model(strategy, IMAGE_SIZE)\n",
    "\n",
    "            # Use RMSprop for more stability\n",
    "            optimizer = tf.keras.optimizers.RMSprop(\n",
    "                learning_rate=5e-5,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-7,\n",
    "                clipnorm=0.5\n",
    "            )\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                metrics=[\n",
    "                    tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.5),\n",
    "                    tf.keras.metrics.AUC(name=\"AUC\", curve='ROC'),\n",
    "                    tf.keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n",
    "                    tf.keras.metrics.Recall(name=\"recall\", thresholds=0.5),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    print(\"PHASE 2: Main training phase\")\n",
    "    # Main training\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        initial_epoch=2,  # Continue from where we left off\n",
    "        epochs=40,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=[\n",
    "            early_stopping_auc,\n",
    "            early_stopping_loss,\n",
    "            checkpoint_callback,\n",
    "            tensorboard_callback,\n",
    "            terminate_on_nan\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    # If training fails, try with even more conservative approach\n",
    "    print(\"Training failed. Attempting with more conservative settings...\")\n",
    "\n",
    "    # Clear memory\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    with strategy.scope():\n",
    "        # Build a simpler model\n",
    "        model = build_model(strategy, IMAGE_SIZE)\n",
    "\n",
    "        # Use SGD for maximum stability\n",
    "        optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=1e-5,\n",
    "            momentum=0.9,\n",
    "            clipnorm=0.1\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "            metrics=[\n",
    "                tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.5),\n",
    "                tf.keras.metrics.AUC(name=\"AUC\", curve='ROC'),\n",
    "                tf.keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n",
    "                tf.keras.metrics.Recall(name=\"recall\", thresholds=0.5),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Try training with the simplified approach\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=30,  # Shorter training\n",
    "            steps_per_epoch=train_steps,\n",
    "            validation_steps=val_steps,\n",
    "            callbacks=[\n",
    "                early_stopping_auc,\n",
    "                early_stopping_loss,\n",
    "                checkpoint_callback,\n",
    "                terminate_on_nan\n",
    "            ],\n",
    "            verbose=1\n",
    "        )"
   ],
   "id": "33263891e52c9bcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save intermediate model with error handling\n",
    "try:\n",
    "    model.save(modelSave_path)\n",
    "    model.save(modelKerasSave_path)\n",
    "    print(f\"Model saved successfully to {modelSave_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "    # Try alternative saving approach\n",
    "    try:\n",
    "        # Save weights only if full model save fails\n",
    "        model.save_weights(f'{modelSave_path}_weights')\n",
    "        print(f\"Model weights saved to {modelSave_path}_weights\")\n",
    "    except:\n",
    "        print(\"Could not save model weights either\")"
   ],
   "id": "a8988d833fd00238"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluation phase with error handling\n",
    "try:\n",
    "    # Load test dataset with stability improvements\n",
    "    test_dataset, test_steps = load_tfrecord_dataset(\n",
    "        test_tfrecords,\n",
    "        batch_size=GLOBAL_BATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        is_training=False\n",
    "    )\n",
    "\n",
    "    # Evaluate with default threshold\n",
    "    print(\"Evaluating with default threshold (0.5):\")\n",
    "    test_results = model.evaluate(\n",
    "        test_dataset,\n",
    "        steps=test_steps,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Test Results:\", dict(zip(model.metrics_names, test_results)))\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "# Force memory cleanup\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ],
   "id": "eb91eb0a2b53b9ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create dataset for threshold tuning with smaller batches\n",
    "try:\n",
    "    threshold_dataset, _ = load_tfrecord_dataset(\n",
    "        val_tfrecords,\n",
    "        batch_size=32,  # Smaller batch size for prediction\n",
    "        image_size=IMAGE_SIZE,\n",
    "        is_training=False\n",
    "    )\n",
    "\n",
    "    # Collect predictions for threshold optimization in smaller batches\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"Collecting predictions for threshold optimization...\")\n",
    "    # Process in smaller chunks to avoid memory issues\n",
    "    for i, (inputs, labels) in enumerate(threshold_dataset):\n",
    "        if i >= val_samples // 32:  # Adjusted for smaller batch size\n",
    "            break\n",
    "        try:\n",
    "            preds = model.predict_on_batch(inputs)\n",
    "\n",
    "            # Check for NaN in predictions\n",
    "            if np.isnan(preds).any():\n",
    "                print(f\"Warning: NaN found in predictions batch {i}\")\n",
    "                continue\n",
    "\n",
    "            # Convert to numpy and store\n",
    "            all_preds.extend(preds.numpy().flatten())\n",
    "            all_labels.extend(labels.numpy().flatten())\n",
    "\n",
    "            # Periodically clear memory\n",
    "            if i % 10 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Find optimal threshold with error handling\n",
    "    print(\"Calculating optimal threshold...\")\n",
    "    if len(all_preds) > 0 and len(all_labels) > 0:\n",
    "        try:\n",
    "            fpr, tpr, thresholds = roc_curve(all_labels, all_preds)\n",
    "            j_scores = tpr - fpr\n",
    "            optimal_idx = np.argmax(j_scores)\n",
    "            optimal_threshold = thresholds[optimal_idx]\n",
    "            optimal_threshold = float(optimal_threshold)\n",
    "\n",
    "            # Calculate metrics with various thresholds\n",
    "            thresholds_to_try = [0.3, 0.4, 0.5, 0.6, 0.7, optimal_threshold]\n",
    "            for threshold in thresholds_to_try:\n",
    "                binary_preds = (np.array(all_preds) >= threshold).astype(int)\n",
    "                precision = sklearn_precision_score(all_labels, binary_preds)\n",
    "                recall = sklearn_recall_score(all_labels, binary_preds)\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "                print(f\"Threshold: {threshold:.4f}, Precision: {precision:.4f}, \"\n",
    "                      f\"Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "            print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating optimal threshold: {e}\")\n",
    "            optimal_threshold = 0.5  # Fallback to default\n",
    "    else:\n",
    "        print(\"No valid predictions collected for threshold optimization\")\n",
    "        optimal_threshold = 0.5  # Use default\n",
    "except Exception as e:\n",
    "    print(f\"Error in threshold tuning phase: {e}\")\n",
    "    optimal_threshold = 0.5  # Use default"
   ],
   "id": "eb9c8416f0ccb331"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Final training phase with focal loss\n",
    "try:\n",
    "    print(\"PHASE 3: Final training with Focal Loss and optimized parameters\")\n",
    "\n",
    "    # Set a moderate learning rate for final training\n",
    "    K.set_value(model.optimizer.learning_rate, 5e-6)  # Conservative learning rate\n",
    "\n",
    "    # Recompile with focal loss and custom metrics\n",
    "    with strategy.scope():\n",
    "        model.compile(\n",
    "            optimizer=model.optimizer,\n",
    "            loss=BinaryFocalCrossentropy(alpha=0.75, gamma=2.0),  # Adjusted focal loss\n",
    "            metrics=[\n",
    "                tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.5),\n",
    "                tf.keras.metrics.AUC(name=\"AUC\", curve='ROC'),\n",
    "                tf.keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n",
    "                tf.keras.metrics.Recall(name=\"recall\", thresholds=0.5),\n",
    "                tf.keras.metrics.Precision(name=\"opt_precision\", thresholds=optimal_threshold),\n",
    "                tf.keras.metrics.Recall(name=\"opt_recall\", thresholds=optimal_threshold),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    # Final training phase with reduced epochs\n",
    "    final_history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=10,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=[\n",
    "            early_stopping_auc,\n",
    "            early_stopping_loss,\n",
    "            checkpoint_callback,\n",
    "            tensorboard_callback,\n",
    "            terminate_on_nan\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save the final model\n",
    "    model.save(modelSave_path)\n",
    "    model.save(modelKerasSave_path)\n",
    "\n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal evaluation results:\")\n",
    "    test_results = model.evaluate(test_dataset, steps=test_steps)\n",
    "    print(\"Test Results:\", dict(zip(model.metrics_names, test_results)))\n",
    "\n",
    "    print(f\"Training complete. Optimal threshold for prediction: {optimal_threshold:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in final training phase: {e}\")\n",
    "    print(\"Training completed with some errors.\")"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
