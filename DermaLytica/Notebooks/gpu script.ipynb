{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.keras.backend as K\n",
    "import gc\n",
    "from sklearn.metrics import precision_recall_curve, precision_score as sklearn_precision_score, \\\n",
    "    recall_score as sklearn_recall_score, roc_curve\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "# GPU Setup\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "# Optional: Use MirroredStrategy for multi-GPU training\n",
    "if len(gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"Training with {strategy.num_replicas_in_sync} GPUs\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()  # Default strategy\n",
    "    print(\"Training with single GPU or CPU\")\n",
    "\n",
    "# Constants\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 64  # Can be larger with GPU compared to TPU\n",
    "\n",
    "# Data constants (same as before)\n",
    "train_samples = 11200\n",
    "train_tfrecords = [f'/content/drive/MyDrive/DermaLyticsAI/small dataset/train.tfrecord']\n",
    "\n",
    "val_samples = 4800\n",
    "val_tfrecords = [f'/content/drive/MyDrive/DermaLyticsAI/small dataset/validation.tfrecord']\n",
    "\n",
    "total_test_samples = 4000\n",
    "test_tfrecords = [f'/content/drive/MyDrive/DermaLyticsAI/small dataset/test.tfrecord']\n",
    "\n",
    "checkpoint_path = f'/content/drive/MyDrive/DermaLyticsAI/small dataset/modelCheckpoint.weights.h5'\n",
    "log_dir = f'/content/drive/MyDrive/DermaLyticsAI/small dataset/logs'\n",
    "modelSave_path = f'/content/drive/MyDrive/DermaLyticsAI/small dataset/model/derma_model.h5'\n",
    "modelKerasSave_path = f'/content/drive/MyDrive/DermaLyticsAI/small dataset/model/derma_model_keras.keras'\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "id": "2a2ab66dce04b58c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_function(proto, image_size, is_training=True):\n",
    "    \"\"\"Parse a single example from TFRecord with additional error handling.\"\"\"\n",
    "    # Define the feature description\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([1], tf.float32),\n",
    "        \"metadata\": tf.io.FixedLenFeature([11], tf.float32),\n",
    "        \"mask\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "\n",
    "    # Parse with error handling\n",
    "    example = tf.io.parse_single_example(proto, feature_description)\n",
    "\n",
    "    # Decode and process the image with error clipping\n",
    "    image = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    image = tf.clip_by_value(tf.cast(image, tf.float32) / 255.0, 0.0, 1.0)  # Ensure proper range\n",
    "\n",
    "    # Decode and process the mask with error clipping\n",
    "    mask = tf.io.decode_jpeg(example[\"mask\"], channels=3)\n",
    "    mask = tf.image.resize(mask, image_size)\n",
    "    mask = tf.clip_by_value(tf.cast(mask, tf.float32) / 255.0, 0.0, 1.0)  # Ensure proper range\n",
    "\n",
    "    if is_training:\n",
    "        # Generate random transformation parameters\n",
    "        seed = tf.random.uniform([2], minval=0, maxval=100, dtype=tf.int32)\n",
    "\n",
    "        # Apply identical flips using the same seed\n",
    "        image = tf.image.stateless_random_flip_left_right(image, seed)\n",
    "        mask = tf.image.stateless_random_flip_left_right(mask, seed)\n",
    "        image = tf.image.stateless_random_flip_up_down(image, seed)\n",
    "        mask = tf.image.stateless_random_flip_up_down(mask, seed)\n",
    "\n",
    "        # Random rotation\n",
    "        flipNumber = tf.random.uniform([], 0, 4, dtype=tf.int32)\n",
    "        image = tf.image.rot90(image, k=flipNumber)\n",
    "        mask = tf.image.rot90(mask, k=flipNumber)\n",
    "\n",
    "    # Extract and normalize metadata\n",
    "    metadata = example[\"metadata\"]\n",
    "    # Clip metadata to reasonable ranges to prevent extreme values\n",
    "    metadata = tf.clip_by_value(metadata, -100.0, 100.0)\n",
    "\n",
    "    # Make sure label is valid binary\n",
    "    label = tf.clip_by_value(example[\"label\"], 0.0, 1.0)\n",
    "\n",
    "    # Return using a dictionary structure\n",
    "    return ({\"image_input\": image, \"mask_input\": mask, \"metadata_input\": metadata}, label)"
   ],
   "id": "cc8090d057910c93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_tfrecord_dataset(tfrecord_paths, batch_size, image_size, epochs=1, is_training=True):\n",
    "    \"\"\"Load TFRecord dataset with enhanced stability and error handling.\"\"\"\n",
    "    # Use the parse function\n",
    "    parse_fn = lambda x: parse_function(x, image_size, is_training)\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = tf.data.TFRecordDataset(\n",
    "        tfrecord_paths,\n",
    "        num_parallel_reads=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(1000)  # Shuffle before batching\n",
    "\n",
    "    # Batch the dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Apply repeat after batching\n",
    "    if is_training:\n",
    "        dataset = dataset.repeat(epochs)\n",
    "\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Calculate steps\n",
    "    if is_training:\n",
    "        steps = train_samples // batch_size\n",
    "    else:\n",
    "        if tfrecord_paths[0] == val_tfrecords[0]:\n",
    "            steps = val_samples // batch_size\n",
    "        else:\n",
    "            steps = total_test_samples // batch_size\n",
    "\n",
    "    return dataset, steps"
   ],
   "id": "c64a6e3039dfb674"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_model(image_size=(224, 224)):\n",
    "    \"\"\"Build model with enhanced numerical stability.\"\"\"\n",
    "    # Set up kernel initializers for stability\n",
    "    kernel_init = tf.keras.initializers.GlorotNormal(seed=42)\n",
    "\n",
    "    # Define the inputs\n",
    "    image_input = tf.keras.Input(shape=(*image_size, 3), name=\"image_input\")\n",
    "    mask_input = tf.keras.Input(shape=(*image_size, 3), name=\"mask_input\")\n",
    "    metadata_input = tf.keras.Input(shape=(11,), name=\"metadata_input\")\n",
    "\n",
    "    # Attention mechanism with stabilized initialization\n",
    "    attention = tf.keras.layers.Conv2D(\n",
    "        1, (1, 1),\n",
    "        activation='sigmoid',\n",
    "        kernel_initializer=kernel_init\n",
    "    )(mask_input)\n",
    "    modulated_image = tf.keras.layers.Multiply()([image_input, attention])\n",
    "\n",
    "    # Load EfficientNetV2B3 with weight initialization for stability\n",
    "    base_model = tf.keras.applications.EfficientNetV2B3(\n",
    "        include_top=False,\n",
    "        input_tensor=modulated_image,\n",
    "        weights=None,  # Start with random weights\n",
    "        classes=2\n",
    "    )\n",
    "\n",
    "    # Freeze initial layers to stabilize training\n",
    "    for layer in base_model.layers[:50]:  # Freeze early layers\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Make later layers trainable\n",
    "    for layer in base_model.layers[50:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Add attention mechanism with stable initialization\n",
    "    attention = tf.keras.layers.Conv2D(\n",
    "        base_model.output.shape[-1],\n",
    "        (1, 1),\n",
    "        activation='sigmoid',\n",
    "        kernel_initializer=kernel_init\n",
    "    )(base_model.output)\n",
    "\n",
    "    weighted_features = tf.keras.layers.Multiply()([base_model.output, attention])\n",
    "\n",
    "    # Process image features with batch normalization for stability\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(weighted_features)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)  # Add dropout for regularization\n",
    "\n",
    "    # Process mask with a small CNN to extract features\n",
    "    mask_features = tf.keras.layers.Conv2D(\n",
    "        16, (3, 3),\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        kernel_initializer=kernel_init\n",
    "    )(mask_input)\n",
    "    mask_features = tf.keras.layers.BatchNormalization()(mask_features)\n",
    "    mask_features = tf.keras.layers.MaxPooling2D()(mask_features)\n",
    "\n",
    "    mask_features = tf.keras.layers.Conv2D(\n",
    "        32, (3, 3),\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        kernel_initializer=kernel_init\n",
    "    )(mask_features)\n",
    "    mask_features = tf.keras.layers.BatchNormalization()(mask_features)\n",
    "    mask_features = tf.keras.layers.GlobalAveragePooling2D()(mask_features)\n",
    "    mask_features = tf.keras.layers.BatchNormalization()(mask_features)\n",
    "\n",
    "    # Split metadata with more stable processing\n",
    "    demographic_features = tf.keras.layers.Lambda(lambda x: x[:, :3])(metadata_input)\n",
    "    location_features = tf.keras.layers.Lambda(lambda x: x[:, 3:])(metadata_input)\n",
    "\n",
    "    # Process location features\n",
    "    location_encoded = tf.keras.layers.Dense(\n",
    "        16,\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=kernel_init,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(location_features)\n",
    "    location_encoded = tf.keras.layers.BatchNormalization()(location_encoded)\n",
    "    location_encoded = tf.keras.layers.Dropout(0.1)(location_encoded)\n",
    "\n",
    "    # Process demographic features\n",
    "    demographic_encoded = tf.keras.layers.Dense(\n",
    "        16,\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=kernel_init,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(demographic_features)\n",
    "    demographic_encoded = tf.keras.layers.BatchNormalization()(demographic_encoded)\n",
    "    demographic_encoded = tf.keras.layers.Dropout(0.1)(demographic_encoded)\n",
    "\n",
    "    # Combine metadata features\n",
    "    metadata_features = tf.keras.layers.Concatenate()([location_encoded, demographic_encoded])\n",
    "    metadata_features = tf.keras.layers.BatchNormalization()(metadata_features)\n",
    "\n",
    "    # Combine all features with BatchNormalization\n",
    "    combined = tf.keras.layers.Concatenate()([x, metadata_features, mask_features])\n",
    "    combined = tf.keras.layers.BatchNormalization()(combined)\n",
    "\n",
    "    # First dense block with residual connection and more regularization\n",
    "    block1 = tf.keras.layers.Dense(\n",
    "        256,\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=kernel_init,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(combined)\n",
    "    block1 = tf.keras.layers.BatchNormalization()(block1)\n",
    "    block1 = tf.keras.layers.Dropout(0.3)(block1)\n",
    "\n",
    "    # Residual connection with the same architecture\n",
    "    block1_res = tf.keras.layers.Dense(\n",
    "        256,\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=kernel_init,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(block1)\n",
    "    block1_res = tf.keras.layers.BatchNormalization()(block1_res)\n",
    "    block1_res = tf.keras.layers.Dropout(0.3)(block1_res)\n",
    "\n",
    "    # Add residual connection for improved gradient flow\n",
    "    block1 = tf.keras.layers.Add()([block1, block1_res])\n",
    "    block1 = tf.keras.layers.BatchNormalization()(block1)\n",
    "\n",
    "    # Second dense block\n",
    "    block2 = tf.keras.layers.Dense(\n",
    "        128,\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=kernel_init,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(block1)\n",
    "    block2 = tf.keras.layers.BatchNormalization()(block2)\n",
    "    block2 = tf.keras.layers.Dropout(0.2)(block2)\n",
    "\n",
    "    # Final classification layer with stable initialization\n",
    "    output = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        activation=\"sigmoid\",\n",
    "        kernel_initializer=kernel_init,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(block2)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[image_input, metadata_input, mask_input], outputs=output)\n",
    "\n",
    "    return model"
   ],
   "id": "1668775e365a2a84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define callbacks for training with additional stability monitoring\n",
    "early_stopping_auc = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_AUC\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode=\"max\"\n",
    ")\n",
    "early_stopping_loss = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=7,\n",
    "    restore_best_weights=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "terminate_on_nan = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    update_freq='epoch'\n",
    ")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_AUC',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Load datasets\n",
    "train_dataset, train_steps = load_tfrecord_dataset(\n",
    "    train_tfrecords,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    epochs=40,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "val_dataset, val_steps = load_tfrecord_dataset(\n",
    "    val_tfrecords,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    epochs=1,  # Only need one epoch for validation\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(f\"Steps per epoch: {train_steps}, Validation steps: {val_steps}\")\n",
    "\n",
    "# Build and compile model - use strategy scope only if using multiple GPUs\n",
    "if len(gpus) > 1:\n",
    "    with strategy.scope():\n",
    "        model = build_model(IMAGE_SIZE)\n",
    "\n",
    "        # Learning rate schedule\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=1e-4,  # Lower initial learning rate\n",
    "            decay_steps=train_steps * 5,\n",
    "            decay_rate=0.95,\n",
    "            staircase=True\n",
    "        )\n",
    "\n",
    "        # Optimizer with enhanced numerical stability\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=lr_schedule,\n",
    "            clipnorm=1.0,        # Gradient clipping\n",
    "            epsilon=1e-7         # Numerical stability\n",
    "        )\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "            metrics=[\n",
    "                tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.5),\n",
    "                tf.keras.metrics.AUC(name=\"AUC\", curve='ROC'),\n",
    "                tf.keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n",
    "                tf.keras.metrics.Recall(name=\"recall\", thresholds=0.5),\n",
    "            ],\n",
    "        )\n",
    "else:\n",
    "    # Single GPU or CPU training\n",
    "    model = build_model(IMAGE_SIZE)\n",
    "\n",
    "    # Learning rate schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-4,\n",
    "        decay_steps=train_steps * 5,\n",
    "        decay_rate=0.95,\n",
    "        staircase=True\n",
    "    )\n",
    "\n",
    "    # Optimizer with enhanced numerical stability\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=lr_schedule,\n",
    "        clipnorm=1.0,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.5),\n",
    "            tf.keras.metrics.AUC(name=\"AUC\", curve='ROC'),\n",
    "            tf.keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n",
    "            tf.keras.metrics.Recall(name=\"recall\", thresholds=0.5),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ],
   "id": "98c120747f37a4a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Phase 1: Initial training with standard loss\n",
    "print(\"PHASE 1: Main training phase\")\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=40,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[\n",
    "        early_stopping_auc,\n",
    "        early_stopping_loss,\n",
    "        checkpoint_callback,\n",
    "        tensorboard_callback,\n",
    "        terminate_on_nan\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Force garbage collection to clear memory\n",
    "gc.collect()\n",
    "K.clear_session()"
   ],
   "id": "55bb6b14888e9183"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the trained model\n",
    "try:\n",
    "    model.save(modelSave_path)\n",
    "    model.save(modelKerasSave_path)\n",
    "    print(f\"Model saved successfully to {modelSave_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "    try:\n",
    "        # Save weights only if full model save fails\n",
    "        model.save_weights(f'{modelSave_path}_weights')\n",
    "        print(f\"Model weights saved to {modelSave_path}_weights\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model weights: {e}\")"
   ],
   "id": "e4613b12df62c719"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluation phase\n",
    "# Load test dataset\n",
    "test_dataset, test_steps = load_tfrecord_dataset(\n",
    "    test_tfrecords,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    epochs=1,\n",
    "    is_training=False\n",
    ")"
   ],
   "id": "e628a14adff1adcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate with default threshold\n",
    "print(\"Evaluating with default threshold (0.5):\")\n",
    "test_results = model.evaluate(\n",
    "    test_dataset,\n",
    "    steps=test_steps,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Test Results:\", dict(zip(model.metrics_names, test_results)))"
   ],
   "id": "72fc1e5d790d1d9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create dataset for threshold tuning\n",
    "threshold_dataset, threshold_steps = load_tfrecord_dataset(\n",
    "    val_tfrecords,\n",
    "    batch_size=32,  # Smaller batch size for prediction\n",
    "    image_size=IMAGE_SIZE,\n",
    "    epochs=1,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "# Collect predictions for threshold optimization\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Collecting predictions for threshold optimization...\")\n",
    "# Predict in batches\n",
    "for i, (inputs, labels) in enumerate(threshold_dataset):\n",
    "    if i >= val_samples // 32:  # Adjusted for batch size\n",
    "        break\n",
    "\n",
    "    preds = model.predict_on_batch(inputs)\n",
    "\n",
    "    # Convert to numpy and store\n",
    "    all_preds.extend(preds.numpy().flatten())\n",
    "    all_labels.extend(labels.numpy().flatten())\n",
    "\n",
    "    # Periodically clear memory\n",
    "    if i % 10 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "# Find optimal threshold\n",
    "print(\"Calculating optimal threshold...\")\n",
    "if len(all_preds) > 0 and len(all_labels) > 0:\n",
    "    try:\n",
    "        fpr, tpr, thresholds = roc_curve(all_labels, all_preds)\n",
    "        j_scores = tpr - fpr\n",
    "        optimal_idx = np.argmax(j_scores)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        optimal_threshold = float(optimal_threshold)\n",
    "\n",
    "        # Calculate metrics with various thresholds\n",
    "        thresholds_to_try = [0.3, 0.4, 0.5, 0.6, 0.7, optimal_threshold]\n",
    "        for threshold in thresholds_to_try:\n",
    "            binary_preds = (np.array(all_preds) >= threshold).astype(int)\n",
    "            precision = sklearn_precision_score(all_labels, binary_preds)\n",
    "            recall = sklearn_recall_score(all_labels, binary_preds)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "            print(f\"Threshold: {threshold:.4f}, Precision: {precision:.4f}, \"\n",
    "                  f\"Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "        print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating optimal threshold: {e}\")\n",
    "        optimal_threshold = 0.5  # Use default\n",
    "else:\n",
    "    print(\"No valid predictions collected for threshold optimization\")\n",
    "    optimal_threshold = 0.5  # Use default\n",
    "\n",
    "# Clear memory before final phase\n",
    "gc.collect()\n",
    "K.clear_session()"
   ],
   "id": "8bc4f875a1a8d368"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Phase 2: Final training with focal loss\n",
    "print(\"PHASE 2: Final training with Focal Loss and optimized parameters\")\n",
    "\n",
    "# Set a moderate learning rate for final training\n",
    "K.set_value(model.optimizer.learning_rate, 5e-6)  # Very conservative learning rate\n",
    "\n",
    "# Recompile with focal loss and custom metrics\n",
    "if len(gpus) > 1:\n",
    "    with strategy.scope():\n",
    "        model.compile(\n",
    "            optimizer=model.optimizer,\n",
    "            loss=BinaryFocalCrossentropy(alpha=0.75, gamma=2.0),  # Adjusted focal loss\n",
    "            metrics=[\n",
    "                tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.5),\n",
    "                tf.keras.metrics.AUC(name=\"AUC\", curve='ROC'),\n",
    "                tf.keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n",
    "                tf.keras.metrics.Recall(name=\"recall\", thresholds=0.5),\n",
    "                tf.keras.metrics.Precision(name=\"opt_precision\", thresholds=optimal_threshold),\n",
    "                tf.keras.metrics.Recall(name=\"opt_recall\", thresholds=optimal_threshold),\n",
    "            ],\n",
    "        )\n",
    "else:\n",
    "    model.compile(\n",
    "        optimizer=model.optimizer,\n",
    "        loss=BinaryFocalCrossentropy(alpha=0.75, gamma=2.0),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.5),\n",
    "            tf.keras.metrics.AUC(name=\"AUC\", curve='ROC'),\n",
    "            tf.keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n",
    "            tf.keras.metrics.Recall(name=\"recall\", thresholds=0.5),\n",
    "            tf.keras.metrics.Precision(name=\"opt_precision\", thresholds=optimal_threshold),\n",
    "            tf.keras.metrics.Recall(name=\"opt_recall\", thresholds=optimal_threshold),\n",
    "        ],\n",
    "    )"
   ],
   "id": "c33c36a90d5b4582"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Final training phase with reduced epochs\n",
    "final_history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,  # Reduced epochs for final phase\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[\n",
    "        early_stopping_auc,\n",
    "        early_stopping_loss,\n",
    "        checkpoint_callback,\n",
    "        tensorboard_callback,\n",
    "        terminate_on_nan\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "try:\n",
    "    model.save(modelSave_path)\n",
    "    model.save(modelKerasSave_path)\n",
    "    print(f\"Final model saved successfully to {modelSave_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final model: {e}\")\n",
    "    try:\n",
    "        model.save_weights(f'{modelSave_path}_final_weights')\n",
    "        print(f\"Final model weights saved to {modelSave_path}_final_weights\")\n",
    "    except:\n",
    "        print(\"Could not save final model weights\")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\nFinal evaluation results:\")\n",
    "test_results = model.evaluate(test_dataset, steps=test_steps)\n",
    "print(\"Test Results:\", dict(zip(model.metrics_names, test_results)))\n",
    "\n",
    "print(f\"Training complete. Optimal threshold for prediction: {optimal_threshold:.4f}\")"
   ],
   "id": "3e83530915f0d4af"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
