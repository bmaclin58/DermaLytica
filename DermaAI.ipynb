{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's gather a few datasets from the ISIC database\n",
    "# Libraries"
   ],
   "id": "a8d9cdd37359abbf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To install the isic Datasets we will use their CLI",
   "id": "c6a162276a572d6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install isic-cli",
   "id": "52a070178fd23f3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Metadata loading & Analysis\n",
    "\n",
    "Before we start looking at the photos, let's gather a few datasets and evaluate the demographic metadata that we have.  For the purposes of this project, we're going to focus on age, gender, and the location of the skin lesion.\n",
    "We'll focus on the following datasets\n",
    "- [BCN 20000](https://www.nature.com/articles/s41597-024-03387-w)\n",
    "- [HAM10000](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T)\n",
    "- [ISIC 2024](https://challenge2024.isic-archive.com/)\n",
    "- [Hospital Italiano de Buenos Aires Skin Lesions](https://www.nature.com/articles/s41597-023-02630-0)"
   ],
   "id": "586f5aa6a450475"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Downloads\n",
    "\n",
    "International Skin Imaging Collaboration (ISIC) archive is a massive resource for images and metadata for our project.  Let's take a quick look at the available data."
   ],
   "id": "d3241c5cf697ce57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!isic collection list",
   "id": "319244184899734b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs(r'E:\\Capstone Skin Cancer Project\\Datasets\\BCN\\MetaData', exist_ok=True)\n",
    "BCN_id = 249\n",
    "!isic metadata download -c {BCN_id} -o \"E:\\Capstone Skin Cancer Project\\Datasets\\BCN\\MetaData\\BCN_Metadata.csv\""
   ],
   "id": "3f315f5f6c0058d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs(r'E:\\Capstone Skin Cancer Project\\Datasets\\BCN\\Image', exist_ok=True)\n",
    "!isic image download --collections 249 \"E:\\Capstone Skin Cancer Project\\Datasets\\BCN\\Image\""
   ],
   "id": "b9f59f3fbf822a23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs(r'E:\\Capstone Skin Cancer Project\\Datasets\\HAM\\MetaData', exist_ok=True)\n",
    "HAM_id = 212\n",
    "!isic metadata download -c {HAM_id} -o \"E:\\Capstone Skin Cancer Project\\Datasets\\HAM\\MetaData\\HAM_Metadata.csv\""
   ],
   "id": "925f6ba3628c0689",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs(r'E:\\Capstone Skin Cancer Project\\Datasets\\HAM\\Image', exist_ok=True)\n",
    "!isic image download --collections 212 \"E:\\Capstone Skin Cancer Project\\Datasets\\HAM\\Image\""
   ],
   "id": "56366b7c355460c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs(r'E:\\Capstone Skin Cancer Project\\Datasets\\BuenosAires\\MetaData', exist_ok=True)\n",
    "BA_id = 390\n",
    "!isic metadata download -c {BA_id} -o \"E:\\Capstone Skin Cancer Project\\Datasets\\BuenosAires\\MetaData\\BA_Metadata.csv\""
   ],
   "id": "dbf4b46d9205e1cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs(r'E:\\Capstone Skin Cancer Project\\Datasets\\BuenosAires\\Image', exist_ok=True)\n",
    "!isic image download --collections {BA_id} \"E:\\Capstone Skin Cancer Project\\Datasets\\BuenosAires\\Image\""
   ],
   "id": "5ad210097b21aeaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs(r'E:\\Capstone Skin Cancer Project\\Datasets\\Braff\\MetaData', exist_ok=True)\n",
    "Braff_id = 410\n",
    "!isic metadata download -c {Braff_id} -o \"E:\\Capstone Skin Cancer Project\\Datasets\\Braff\\MetaData\\Braff_Metadata.csv\""
   ],
   "id": "37733afa2d76637f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs(r'E:\\Capstone Skin Cancer Project\\Datasets\\Braff\\Image', exist_ok=True)\n",
    "!isic image download --collections {Braff_id} \"E:\\Capstone Skin Cancer Project\\Datasets\\Braff\\Image\""
   ],
   "id": "80a233b50a03d792",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs(r'E:\\Capstone Skin Cancer Project\\Datasets\\Melo\\MetaData', exist_ok=True)\n",
    "melo_id = 294\n",
    "!isic metadata download -c {melo_id} -o \"E:\\Capstone Skin Cancer Project\\Datasets\\Melo\\MetaData\\Melo_Metadata.csv\""
   ],
   "id": "ccab3208809d1dfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs(r'E:\\Capstone Skin Cancer Project\\Datasets\\Melo\\Image', exist_ok=True)\n",
    "!isic image download --collections {melo_id} \"E:\\Capstone Skin Cancer Project\\Datasets\\Melo\\Image\""
   ],
   "id": "a39bd218245c6176",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets take a look at the data columns that we currently have, then clean the data up so we can keep the items that we will be looking for to see if there's any correlation between the data points and cancer.",
   "id": "83e60d82cc22151c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BCN = pd.read_csv(r'E:\\Capstone Skin Cancer Project\\Datasets\\BCN\\MetaData\\BCN_Metadata.csv')\n",
    "BCN.head(5)"
   ],
   "id": "1992f30994541104",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HAM = pd.read_csv(r'E:\\Capstone Skin Cancer Project\\Datasets\\HAM\\MetaData\\HAM_Metadata.csv')\n",
    "HAM.head(5)"
   ],
   "id": "df9b9addf25afbbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BA = pd.read_csv(r'E:\\Capstone Skin Cancer Project\\Datasets\\BuenosAires\\MetaData\\BA_Metadata.csv', low_memory=False)\n",
    "BA.head(5)"
   ],
   "id": "86ed3be7cc4f2765",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BRAFF = pd.read_csv(r'E:\\Capstone Skin Cancer Project\\Datasets\\Braff\\MetaData\\Braff_Metadata.csv')\n",
    "BRAFF.head(5)"
   ],
   "id": "afe39f682383f0c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MELO = pd.read_csv(r'E:\\Capstone Skin Cancer Project\\Datasets\\Melo\\MetaData\\Melo_Metadata.csv', low_memory=False)\n",
    "MELO.head(5)"
   ],
   "id": "1f40518d176f6ba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After all data has been read, we'll make a function to format all of the data",
   "id": "36a1f6048e70e095"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns_to_keep = ['isic_id', 'age_approx', 'sex', 'anatom_site_general', 'diagnosis_1', 'diagnosis']\n",
    "translation_dict = {\n",
    "\t\t'upper extremity': 'Shoulders & Arms',\n",
    "\t\t'head/neck':       'Head & Neck',\n",
    "\t\t'palms/soles':     'Palms & Soles',\n",
    "\t\t'anterior torso':  'Front Torso',\n",
    "\t\t'lower extremity': 'Legs',\n",
    "\t\t'oral/genital':    'Mouth & Groin',\n",
    "\t\t'posterior torso': 'Back',\n",
    "\t\t'lateral torso':   'Side Torso (Ribs)', }\n",
    "\n",
    "\n",
    "def dataFormatting(table):\n",
    "\tif 'diagnosis' not in table.columns and 'diagnosis_2' in table.columns:\n",
    "\t\ttable['diagnosis'] = table['diagnosis_2']\n",
    "\n",
    "\t# Update the 'diagnosis' column if it's blank by using the value from 'diagnosis_1'\n",
    "\ttable.loc[table['diagnosis'].isna(), 'diagnosis'] = table['diagnosis_1']\n",
    "\n",
    "\t# Normalize the text in the 'diagnosis' column for consistent filtering\n",
    "\ttable['diagnosis'] = table['diagnosis'].str.strip().str.lower()\n",
    "\n",
    "\t# Remove rows where diagnosis_1 (Benign/Malignant) is \"Indeterminate\"\n",
    "\ttable = table[table['diagnosis_1'].str.strip() != \"Indeterminate\"]\n",
    "\n",
    "\ttable = table.dropna(subset=columns_to_keep)  # Drop any columns that don't have the data we need.\n",
    "\n",
    "\tformatted_table = table[columns_to_keep]\n",
    "\tformatted_table = formatted_table.rename(columns={'age_approx':          'Age',\n",
    "\t                                                  'sex':                 'Gender',\n",
    "\t                                                  'anatom_site_general': 'Location',\n",
    "\t                                                  'diagnosis_1':         'Benign/Malignant',\n",
    "\t                                                  'diagnosis':           'Diagnosis'\n",
    "\t                                                  })\n",
    "\tformatted_table['Location'] = formatted_table['Location'].replace(translation_dict)\n",
    "\tformatted_table['Age'] = formatted_table['Age'].astype(int)\n",
    "\treturn formatted_table"
   ],
   "id": "d0b72bcb5451b8d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets make one more helper function to find out how many rows are in each table",
   "id": "714f79aa3c3e2454"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def rowCount(tables):\n",
    "\tfor name, table in tables.items():\n",
    "\t\tprint(f\"{name} has {table.shape[0]} rows\")\n",
    "\ttotal_rows = sum(table.shape[0] for table in tables.values())\n",
    "\tprint(f\"There are a total of {total_rows} rows in all tables\")"
   ],
   "id": "1eec5eab8b4c5d85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "formatted_BCN = dataFormatting(BCN)\n",
    "formatted_BCN.head(5)"
   ],
   "id": "9237012141154a12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "formatted_HAM = dataFormatting(HAM)\n",
    "formatted_HAM.head(5)"
   ],
   "id": "90717bd31007b014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "formatted_MELO = dataFormatting(MELO)\n",
    "formatted_MELO.head(5)"
   ],
   "id": "f13363100c9aa54b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "formatted_BA = dataFormatting(BA)\n",
    "formatted_BA.head(5)"
   ],
   "id": "5d85818dc6f1b120",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "formatted_BRAFF = dataFormatting(BRAFF)\n",
    "formatted_BRAFF.head(5)"
   ],
   "id": "3ba7c81d45ebf9be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tables = {\n",
    "\t\t\"BCN\":   formatted_BCN,\n",
    "\t\t\"HAM\":   formatted_HAM,\n",
    "\t\t\"MELO\":  formatted_MELO,\n",
    "\t\t\"BRAFF\": formatted_BRAFF,\n",
    "\t\t\"BA\":    formatted_BA\n",
    "}\n",
    "rowCount(tables)"
   ],
   "id": "6df13c44d1ffe678",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_data_duplicated = pd.concat([formatted_BCN, formatted_HAM, formatted_MELO, formatted_BRAFF, formatted_BA],\n",
    "                                ignore_index=True)\n",
    "all_data = all_data_duplicated.drop_duplicates(subset='isic_id')\n",
    "all_data.to_csv(r'E:\\Capstone Skin Cancer Project\\Datasets\\all_datasets_combined.csv', index=False)\n",
    "print(f'There are {len(all_data)} rows in the combined dataset')\n",
    "duplicate_isic_ids = all_data[all_data['isic_id'].duplicated()]\n",
    "\n",
    "# Output result\n",
    "if duplicate_isic_ids.empty:\n",
    "\tprint(\"All isic_id values are unique.\")\n",
    "else:\n",
    "\tprint(f\"The following isic_id values are duplicated:\\n{duplicate_isic_ids}\")"
   ],
   "id": "4752d9a64f230978",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group data by 'Gender' and 'Benign/Malignant' and count occurrences\n",
    "gender_bm_counts = all_data.groupby([\n",
    "\t\t'Gender',\n",
    "\t\t'Benign/Malignant']).size().reset_index(name='Count')\n",
    "\n",
    "# Create the bar plot\n",
    "ax = sns.barplot(\n",
    "\t\tdata=gender_bm_counts,\n",
    "\t\tx='Benign/Malignant',\n",
    "\t\ty='Count',\n",
    "\t\thue='Gender',\n",
    "\t\tpalette=['#FBE8A1', '#FFDCF4']\n",
    ")\n",
    "\n",
    "# Add bar labels\n",
    "for bar in ax.patches:\n",
    "\tcount = int(bar.get_height())\n",
    "\tax.text(\n",
    "\t\t\tbar.get_x() + bar.get_width() / 2,\n",
    "\t\t\tcount,\n",
    "\t\t\tf\"{count}\",\n",
    "\t\t\tha='center',\n",
    "\t\t\tva='bottom',\n",
    "\t\t\tfontsize=12,\n",
    "\t\t\tcolor='black'\n",
    "\t)\n",
    "\n",
    "plt.legend(title=\"Gender\", loc='upper right')\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of Gender per Benign and Malignant Categories\")\n",
    "\n",
    "plt.show()"
   ],
   "id": "e29b18a75a08dbd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "malignant_data = all_data[all_data['Benign/Malignant'] == 'Malignant']\n",
    "age_gender_counts = malignant_data.groupby(['Age', 'Gender']).size().reset_index(name='Count')\n",
    "\n",
    "male_counts = age_gender_counts[age_gender_counts['Gender'] == 'male']\n",
    "female_counts = age_gender_counts[age_gender_counts['Gender'] == 'female']\n",
    "\n",
    "plt.plot(male_counts['Age'], male_counts['Count'], label='Male', color='blue')\n",
    "plt.plot(female_counts['Age'], female_counts['Count'], label='Female', color='pink')\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count of Malignant Cases')\n",
    "plt.title('Count of Malignant Cases by Age and Gender')\n",
    "plt.legend(title='Gender')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "bbc3c92d97bd59a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "location_gender_counts = malignant_data.groupby(['Location', 'Gender']).size().reset_index(name='Count')\n",
    "\n",
    "# Separate the counts by gender\n",
    "male_location_counts = location_gender_counts[location_gender_counts['Gender'] == 'male']\n",
    "female_location_counts = location_gender_counts[location_gender_counts['Gender'] == 'female']\n",
    "\n",
    "# Sort each by count in descending order\n",
    "male_location_counts = male_location_counts.sort_values(by='Count', ascending=False)\n",
    "female_location_counts = female_location_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "ax = sns.barplot(\n",
    "\t\tdata=location_gender_counts,\n",
    "\t\tx='Location',\n",
    "\t\ty='Count',\n",
    "\t\thue='Gender',\n",
    "\t\tpalette=['blue', 'pink']\n",
    ")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.legend(title=\"Gender\", loc='upper right')\n",
    "plt.xlabel(\"Location\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of Malignant Diagnoses by Location and Gender\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "max_length = max(len(male_location_counts), len(female_location_counts))\n",
    "\n",
    "print(f\"{'Male:':<30}{'Female:'}\")\n",
    "\n",
    "for i in range(max_length):\n",
    "\tmale_str = f\"{male_location_counts.iloc[i]['Location']}: {male_location_counts.iloc[i]['Count']}\" if i < len(\n",
    "\t\t\tmale_location_counts) else \"\"\n",
    "\tfemale_str = f\"{female_location_counts.iloc[i]['Location']}: {female_location_counts.iloc[i]['Count']}\" if i < len(\n",
    "\t\t\tfemale_location_counts) else \"\"\n",
    "\tprint(f\"{male_str:<30}{female_str}\")"
   ],
   "id": "8c4e235cd6b32501",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Benign_data = all_data[all_data['Benign/Malignant'] == 'Benign']\n",
    "age_gender_counts = Benign_data.groupby(['Age', 'Gender']).size().reset_index(name='Count')\n",
    "\n",
    "male_counts = age_gender_counts[age_gender_counts['Gender'] == 'male']\n",
    "female_counts = age_gender_counts[age_gender_counts['Gender'] == 'female']\n",
    "\n",
    "plt.plot(male_counts['Age'], male_counts['Count'], label='Male', color='blue')\n",
    "plt.plot(female_counts['Age'], female_counts['Count'], label='Female', color='pink')\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count of Benign Cases')\n",
    "plt.title('Count of Benign Cases by Age and Gender')\n",
    "plt.legend(title='Gender')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "7a3570373ab0b8ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Benign_location_gender_counts = Benign_data.groupby(['Location', 'Gender']).size().reset_index(name='Count')\n",
    "\n",
    "# Separate the counts by gender\n",
    "Benign_male_location_counts = Benign_location_gender_counts[Benign_location_gender_counts['Gender'] == 'male']\n",
    "Benign_female_location_counts = Benign_location_gender_counts[Benign_location_gender_counts['Gender'] == 'female']\n",
    "\n",
    "# Sort each by count in descending order\n",
    "Benign_male_location_counts = Benign_male_location_counts.sort_values(by='Count', ascending=False)\n",
    "Benign_female_location_counts = Benign_female_location_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "ax = sns.barplot(\n",
    "\t\tdata=Benign_location_gender_counts,\n",
    "\t\tx='Location',\n",
    "\t\ty='Count',\n",
    "\t\thue='Gender',\n",
    "\t\tpalette=['blue', 'pink']\n",
    ")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.legend(title=\"Gender\", loc='upper right')\n",
    "plt.xlabel(\"Location\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of Benign Diagnoses by Location and Gender\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "max_length = max(len(Benign_male_location_counts), len(Benign_female_location_counts))\n",
    "\n",
    "print(f\"{'Male:':<30}{'Female:'}\")\n",
    "\n",
    "for i in range(max_length):\n",
    "\tmale_str = f\"{Benign_male_location_counts.iloc[i]['Location']}: {Benign_male_location_counts.iloc[i]['Count']}\" if i < len(\n",
    "\t\t\tBenign_male_location_counts) else \"\"\n",
    "\tfemale_str = f\"{Benign_female_location_counts.iloc[i]['Location']}: {Benign_female_location_counts.iloc[i]['Count']}\" if i < len(\n",
    "\t\t\tBenign_female_location_counts) else \"\"\n",
    "\tprint(f\"{male_str:<30}{female_str}\")"
   ],
   "id": "da5c18658f9060c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "metadata_path = r\"E:\\Capstone Skin Cancer Project\\Datasets\\all_datasets_combined.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "all_data = pd.read_csv(metadata_path)\n",
    "\n",
    "categorical_features = ['Gender', 'Location', 'Diagnosis']\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "encoded_features = encoder.fit_transform(all_data[categorical_features])\n",
    "\n",
    "# Convert to DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# Concatenate encoded metadata with the original DataFrame\n",
    "all_data_encoded = pd.concat([all_data, encoded_df], axis=1)\n",
    "\n",
    "# Drop original categorical columns\n",
    "all_data_encoded.drop(columns=categorical_features, inplace=True)\n",
    "\n",
    "all_data_encoded.head()"
   ],
   "id": "d83147da77b30e46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select numerical features\n",
    "numerical_features = ['Age']\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize numerical columns\n",
    "all_data_encoded[numerical_features] = scaler.fit_transform(all_data_encoded[numerical_features])\n",
    "\n",
    "# Display normalized dataset\n",
    "all_data_encoded.head()"
   ],
   "id": "2019bcc01da39d8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert labels to numerical format\n",
    "all_data_encoded['label'] = all_data_encoded['Benign/Malignant'].map({'Benign': 0, 'Malignant': 1})\n",
    "\n",
    "# Drop the original label column\n",
    "all_data_encoded.drop(columns=['Benign/Malignant'], inplace=True)\n",
    "\n",
    "# Display the dataset with labels\n",
    "all_data_encoded.head()"
   ],
   "id": "6c1c72073c87b21b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_data_encoded.to_csv(r\"E:\\Capstone Skin Cancer Project\\Datasets\\all_data_encoded.csv\", index=False)",
   "id": "a76a30027f0cbfbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_dirs = [\n",
    "\t\tr\"E:\\Capstone Skin Cancer Project\\Datasets\\BCN\\Image\",\n",
    "\t\tr\"E:\\Capstone Skin Cancer Project\\Datasets\\Braff\\Image\",\n",
    "\t\tr\"E:\\Capstone Skin Cancer Project\\Datasets\\BuenosAires\\Image\",\n",
    "\t\tr\"E:\\Capstone Skin Cancer Project\\Datasets\\HAM\\Image\",\n",
    "\t\tr\"E:\\Capstone Skin Cancer Project\\Datasets\\Melo\\Image\"\n",
    "]\n",
    "# Store all available images in a set\n",
    "image_files = set()\n",
    "for directory in image_dirs:\n",
    "\tfor file in os.listdir(directory):\n",
    "\t\tif file.endswith(\".jpg\"):\n",
    "\t\t\timage_files.add(file)\n",
    "\n",
    "# Check for missing images\n",
    "missing_images = all_data_encoded[~all_data_encoded['isic_id'].apply(lambda x: f\"{x}.jpg\").isin(image_files)]\n",
    "\n",
    "if not missing_images.empty:\n",
    "\tprint(\"ğŸš¨ Missing images found! Listing them below:\")\n",
    "\tprint(missing_images['isic_id'].tolist())\n"
   ],
   "id": "931f01f92ad9b103",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Store full image paths in the metadata\n",
    "def find_image_path(isic_id):\n",
    "\tfilename = f\"{isic_id}.jpg\"\n",
    "\tfor directory in image_dirs:\n",
    "\t\tfull_path = os.path.join(directory, filename)\n",
    "\t\tif os.path.exists(full_path):\n",
    "\t\t\treturn full_path\n",
    "\treturn None"
   ],
   "id": "256e64e782f5621b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add image path column\n",
    "all_data_encoded['image_path'] = all_data_encoded['isic_id'].apply(find_image_path)"
   ],
   "id": "18b3d6ccb0cfb9b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop missing images from the dataset\n",
    "all_data_encoded = all_data_encoded.dropna(subset=['image_path'])"
   ],
   "id": "ad7f3ba16e080f53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save updated dataset with image paths\n",
    "all_data_encoded.to_csv(r\"E:\\Capstone Skin Cancer Project\\Datasets\\all_data_with_paths.csv\", index=False)\n",
    "print(\"âœ… Image verification complete! Updated dataset saved.\")"
   ],
   "id": "8b71d1682914b195",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:40:52.292302Z",
     "start_time": "2025-02-23T05:40:51.192444Z"
    }
   },
   "cell_type": "code",
   "source": "all_data_encoded = pd.read_csv(r\"E:\\Capstone Skin Cancer Project\\Datasets\\all_data_with_paths.csv\")",
   "id": "902a60c3118595c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:48:13.570078Z",
     "start_time": "2025-02-23T05:40:55.109105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Store resolutions\n",
    "resolutions = []\n",
    "\n",
    "for image_path in all_data_encoded['image_path']:\n",
    "\timg = cv2.imread(image_path)\n",
    "\tif img is not None:\n",
    "\t\theight, width, _ = img.shape\n",
    "\t\tresolutions.append((width, height))\n",
    "\n",
    "# Convert to NumPy array\n",
    "resolutions = np.array(resolutions)\n",
    "\n",
    "# Find min/max resolution\n",
    "min_width, min_height = resolutions.min(axis=0)\n",
    "max_width, max_height = resolutions.max(axis=0)\n",
    "\n",
    "print(f\"ğŸ“ Min Resolution: {min_width}x{min_height}\")\n",
    "print(f\"ğŸ“ Max Resolution: {max_width}x{max_height}\")"
   ],
   "id": "6886c17d6b81543a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Min Resolution: 41x41\n",
      "ğŸ“ Max Resolution: 7360x5184\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:48:16.896027Z",
     "start_time": "2025-02-23T06:48:16.497608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset\n",
    "train_data, test_data = train_test_split(all_data_encoded, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"âœ… Train samples: {len(train_data)}\")\n",
    "print(f\"âœ… Test samples: {len(test_data)}\")\n"
   ],
   "id": "414cb8d47b4b3e96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train samples: 332372\n",
      "âœ… Test samples: 83093\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets start building the model",
   "id": "f62f7c923762b729"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:21:33.667328Z",
     "start_time": "2025-02-23T05:21:32.585582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create the model using MobileNetV2 directly\n",
    "base_model = tf.keras.applications.MobileNetV3Large(\n",
    "\t\tinput_shape=(224, 224, 3),\n",
    "\t\tinclude_top=False,\n",
    "\t\tweights='imagenet'\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "\t\tbase_model,\n",
    "\t\ttf.keras.layers.GlobalAveragePooling2D(),\n",
    "\t\ttf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ],
   "id": "347266a7a244ec04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n",
      "\u001B[1m12683000/12683000\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_9\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0mâ”ƒ\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0mâ”ƒ\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ MobileNetV3Large (\u001B[38;5;33mFunctional\u001B[0m)   â”‚ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m960\u001B[0m)      â”‚     \u001B[38;5;34m2,996,352\u001B[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_1      â”‚ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m960\u001B[0m)            â”‚             \u001B[38;5;34m0\u001B[0m â”‚\n",
       "â”‚ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_8 (\u001B[38;5;33mDense\u001B[0m)                 â”‚ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m)              â”‚         \u001B[38;5;34m1,922\u001B[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ MobileNetV3Large (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,996,352</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_1      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,922</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m2,998,274\u001B[0m (11.44 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,998,274</span> (11.44 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m2,973,874\u001B[0m (11.34 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,973,874</span> (11.34 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m24,400\u001B[0m (95.31 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,400</span> (95.31 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets test some methods to apply a masking method for the segmentation model",
   "id": "34f2bc266016792e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:22:23.934720Z",
     "start_time": "2025-02-23T06:22:23.929077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def create_mask_otsu_test(image):\n",
    "    \"\"\"\n",
    "    Create an enhanced binary mask using an improved preprocessing pipeline:\n",
    "    1. Convert to grayscale.\n",
    "    2. Enhance contrast using CLAHE.\n",
    "    3. Denoise with a bilateral filter.\n",
    "    4. Sharpen using an unsharp mask filter.\n",
    "    5. Optionally smooth with a Gaussian blur.\n",
    "    6. Apply Otsu's thresholding.\n",
    "    7. Clean up with morphological operations.\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Enhance local contrast using CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "\n",
    "    # Use a bilateral filter to reduce noise while preserving edges\n",
    "    denoised = cv2.bilateralFilter(enhanced, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # Sharpen the image using an unsharp masking kernel\n",
    "    sharpening_kernel = np.array([[-1, -1, -1],\n",
    "                                  [-1,  9, -1],\n",
    "                                  [-1, -1, -1]])\n",
    "    sharpened = cv2.filter2D(denoised, -1, sharpening_kernel)\n",
    "\n",
    "    # Optional: Apply Gaussian Blur to reduce any high-frequency artifacts\n",
    "    blurred = cv2.GaussianBlur(sharpened, (5, 5), 0)\n",
    "\n",
    "    # Apply Otsu's thresholding to create the binary mask\n",
    "    _, mask = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Use morphological opening to remove small noise artifacts from the mask\n",
    "    kernel_morph = np.ones((3, 3), np.uint8)\n",
    "    mask_clean = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_morph, iterations=1)\n",
    "\n",
    "    return mask_clean"
   ],
   "id": "419838e5409db4a0",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:06:35.612293Z",
     "start_time": "2025-02-23T06:06:35.604171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_mask_watershed(image):\n",
    "\t\"\"\"Create mask using Watershed segmentation\"\"\"\n",
    "\t# Convert to grayscale\n",
    "\tgray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\t# Apply Gaussian blur\n",
    "\tblurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\t# Otsu's thresholding for markers\n",
    "\t_, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "\t# Noise removal using morphological operations\n",
    "\tkernel = np.ones((3, 3), np.uint8)\n",
    "\topening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "\t# Sure background area\n",
    "\tsure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "\t# Finding sure foreground area\n",
    "\tdist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "\t_, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "\tsure_fg = np.uint8(sure_fg)\n",
    "\n",
    "\t# Finding unknown region\n",
    "\tunknown = cv2.subtract(sure_bg, sure_fg)\n",
    "\n",
    "\t# Marker labelling\n",
    "\t_, markers = cv2.connectedComponents(sure_fg)\n",
    "\tmarkers = markers + 1\n",
    "\tmarkers[unknown == 255] = 0\n",
    "\n",
    "\t# Apply watershed\n",
    "\tmarkers = cv2.watershed(image, markers)\n",
    "\tmask = np.zeros_like(gray)\n",
    "\tmask[markers > 1] = 255\n",
    "\treturn mask"
   ],
   "id": "9669dd3095fcf881",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:06:55.102090Z",
     "start_time": "2025-02-23T06:06:55.097504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_mask_adaptive(image):\n",
    "\t\"\"\"Create mask using adaptive thresholding\"\"\"\n",
    "\tgray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\tblurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\tmask = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "\t                             cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\treturn mask"
   ],
   "id": "1d263d39b6aa9212",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:07:05.140916Z",
     "start_time": "2025-02-23T06:07:05.133917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_and_save_results(image_path, output_dir):\n",
    "\t\"\"\"Process an image with different segmentation methods and save results\"\"\"\n",
    "\t# Read and preprocess image\n",
    "\timage = cv2.imread(str(image_path))\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t# Create output directory if it doesn't exist\n",
    "\toutput_dir = Path(output_dir)\n",
    "\toutput_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\t# Get base filename\n",
    "\tbase_name = image_path.stem\n",
    "\n",
    "\t# Apply different segmentation methods\n",
    "\tmasks = {\n",
    "\t\t\t'otsu':      create_mask_otsu_test(image),\n",
    "\t\t\t'watershed': create_mask_watershed(image),\n",
    "\t\t\t'adaptive':  create_mask_adaptive(image)\n",
    "\t}\n",
    "\n",
    "\t# Create figure to display results\n",
    "\tplt.figure(figsize=(15, 5))\n",
    "\n",
    "\t# Plot original image\n",
    "\tplt.subplot(141)\n",
    "\tplt.imshow(image)\n",
    "\tplt.title('Original')\n",
    "\tplt.axis('off')\n",
    "\n",
    "\t# Plot masks\n",
    "\tfor i, (method, mask) in enumerate(masks.items(), 2):\n",
    "\t\tplt.subplot(1, 4, i)\n",
    "\t\tplt.imshow(mask, cmap='gray')\n",
    "\t\tplt.title(method.capitalize())\n",
    "\t\tplt.axis('off')\n",
    "\n",
    "\t\t# Save individual mask\n",
    "\t\tcv2.imwrite(str(output_dir / f\"{base_name}_{method}_mask.png\"), mask)\n",
    "\n",
    "\t# Save comparison figure\n",
    "\tplt.savefig(str(output_dir / f\"{base_name}_comparison.png\"))\n",
    "\tplt.close()"
   ],
   "id": "cd73f696bdf897ff",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def maskTest():\n",
    "\t# Define paths\n",
    "\tvalidation_dir = Path(r\"E:\\Capstone Skin Cancer Project\\Datasets\\All Images\\Mask Validation\")\n",
    "\toutput_dir = validation_dir / \"segmentation_results\"\n",
    "\n",
    "\t# Process each image in the validation directory\n",
    "\tfor image_path in validation_dir.glob(\"*.jpg\"):\n",
    "\t\ttry:\n",
    "\t\t\tprocess_and_save_results(image_path, output_dir)\n",
    "\t\t\tprint(f\"Processed {image_path.name}\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error processing {image_path.name}: {str(e)}\")\n",
    "\tprint(\"Segmentation results saved.\")"
   ],
   "id": "6ebca3371aac49dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:23:21.609859Z",
     "start_time": "2025-02-23T06:22:37.066928Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ISIC_0000466.jpg\n",
      "Processed ISIC_0000469.jpg\n",
      "Processed ISIC_0000482.jpg\n",
      "Processed ISIC_0000484.jpg\n",
      "Processed ISIC_0000487.jpg\n",
      "Processed ISIC_0000502.jpg\n",
      "Processed ISIC_0000511.jpg\n",
      "Processed ISIC_0000513.jpg\n",
      "Processed ISIC_0000516.jpg\n",
      "Processed ISIC_0000517.jpg\n",
      "Processed ISIC_0000518.jpg\n",
      "Processed ISIC_0000519.jpg\n",
      "Processed ISIC_0000520.jpg\n",
      "Processed ISIC_0000521.jpg\n",
      "Processed ISIC_0000522.jpg\n",
      "Processed ISIC_0000526.jpg\n",
      "Processed ISIC_0000531.jpg\n",
      "Processed ISIC_0000533.jpg\n",
      "Processed ISIC_0000547.jpg\n",
      "Processed ISIC_0000548.jpg\n",
      "Processed ISIC_0000549.jpg\n",
      "Processed ISIC_0000550.jpg\n",
      "Processed ISIC_0000551.jpg\n",
      "Processed ISIC_0000552.jpg\n",
      "Processed ISIC_0053528.jpg\n",
      "Processed ISIC_0053530.jpg\n",
      "Processed ISIC_0053531.jpg\n",
      "Processed ISIC_0053549.jpg\n",
      "Processed ISIC_0053599.jpg\n",
      "Processed ISIC_0053675.jpg\n",
      "Processed ISIC_0053758.jpg\n",
      "Processed ISIC_0053759.jpg\n",
      "Processed ISIC_0053760.jpg\n",
      "Processed ISIC_0053761.jpg\n",
      "Processed ISIC_0053762.jpg\n",
      "Processed ISIC_0053763.jpg\n",
      "Processed ISIC_0053764.jpg\n",
      "Processed ISIC_0053765.jpg\n",
      "Processed ISIC_0076387.jpg\n",
      "Processed ISIC_0100824.jpg\n",
      "Processed ISIC_0101747.jpg\n",
      "Processed ISIC_0113272.jpg\n",
      "Processed ISIC_0145541.jpg\n",
      "Processed ISIC_0147040.jpg\n",
      "Processed ISIC_0364937.jpg\n",
      "Processed ISIC_0370347.jpg\n",
      "Processed ISIC_0391803.jpg\n",
      "Processed ISIC_0410095.jpg\n",
      "Processed ISIC_0433723.jpg\n",
      "Processed ISIC_0507042.jpg\n",
      "Processed ISIC_0521273.jpg\n",
      "Processed ISIC_0536538.jpg\n",
      "Processed ISIC_0558386.jpg\n",
      "Processed ISIC_0597500.jpg\n",
      "Processed ISIC_0604286.jpg\n",
      "Processed ISIC_0633857.jpg\n",
      "Processed ISIC_0634464.jpg\n",
      "Processed ISIC_0644793.jpg\n",
      "Processed ISIC_0681211.jpg\n",
      "Processed ISIC_0685166.jpg\n",
      "Processed ISIC_0713136.jpg\n",
      "Processed ISIC_0723312.jpg\n",
      "Processed ISIC_0753459.jpg\n",
      "Processed ISIC_0760117.jpg\n",
      "Processed ISIC_0781432.jpg\n",
      "Processed ISIC_0784383.jpg\n",
      "Processed ISIC_0787397.jpg\n",
      "Processed ISIC_0843685.jpg\n",
      "Processed ISIC_0851266.jpg\n",
      "Processed ISIC_0860777.jpg\n",
      "Processed ISIC_0896466.jpg\n",
      "Processed ISIC_0947721.jpg\n",
      "Processed ISIC_0947777.jpg\n",
      "Processed ISIC_0966167.jpg\n",
      "Processed ISIC_0976683.jpg\n",
      "Processed ISIC_0982798.jpg\n",
      "Processed ISIC_1010263.jpg\n",
      "Processed ISIC_1028236.jpg\n",
      "Processed ISIC_1036062.jpg\n",
      "Processed ISIC_1043270.jpg\n",
      "Processed ISIC_1049895.jpg\n",
      "Processed ISIC_1051054.jpg\n",
      "Processed ISIC_1077488.jpg\n",
      "Processed ISIC_1164144.jpg\n",
      "Processed ISIC_1706019.jpg\n",
      "Processed ISIC_1712676.jpg\n",
      "Processed ISIC_1714114.jpg\n",
      "Processed ISIC_1740900.jpg\n",
      "Processed ISIC_1771167.jpg\n",
      "Processed ISIC_1772950.jpg\n",
      "Processed ISIC_1779693.jpg\n",
      "Processed ISIC_1783194.jpg\n",
      "Processed ISIC_1812976.jpg\n",
      "Processed ISIC_1844624.jpg\n",
      "Processed ISIC_1871644.jpg\n",
      "Processed ISIC_1885391.jpg\n",
      "Processed ISIC_1887586.jpg\n",
      "Processed ISIC_1892168.jpg\n",
      "Processed ISIC_1918354.jpg\n",
      "Processed ISIC_1925536.jpg\n",
      "Processed ISIC_1998024.jpg\n",
      "Processed ISIC_2005504.jpg\n",
      "Processed ISIC_2037396.jpg\n",
      "Processed ISIC_8178504.jpg\n",
      "Processed ISIC_8187856.jpg\n",
      "Processed ISIC_8207032.jpg\n",
      "Processed ISIC_8221988.jpg\n",
      "Processed ISIC_8253592.jpg\n",
      "Processed ISIC_8264762.jpg\n"
     ]
    }
   ],
   "execution_count": 31,
   "source": "maskTest()",
   "id": "219606bffe3af34e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:52:14.736963Z",
     "start_time": "2025-02-23T06:52:14.732843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def create_mask_otsu(image):\n",
    "    \"\"\"\n",
    "    Create an enhanced binary mask using an improved preprocessing pipeline:\n",
    "    1. Convert to grayscale.\n",
    "    2. Enhance contrast using CLAHE.\n",
    "    3. Denoise with a bilateral filter.\n",
    "    4. Sharpen using an unsharp mask filter.\n",
    "    5. Optionally smooth with a Gaussian blur.\n",
    "    6. Apply Otsu's thresholding.\n",
    "    7. Clean up with morphological operations.\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale\n",
    "     # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Enhance local contrast using CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "\n",
    "    # Use a bilateral filter to reduce noise while preserving edges\n",
    "    denoised = cv2.bilateralFilter(enhanced, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # Sharpen the image using an unsharp masking kernel\n",
    "    sharpening_kernel = np.array([[-1, -1, -1],\n",
    "                                 [-1,  9, -1],\n",
    "                                 [-1, -1, -1]])\n",
    "    sharpened = cv2.filter2D(denoised, -1, sharpening_kernel)\n",
    "\n",
    "    # Optional: Apply Gaussian Blur to reduce any high-frequency artifacts\n",
    "    blurred = cv2.GaussianBlur(sharpened, (5, 5), 0)\n",
    "\n",
    "    # Apply Otsu's thresholding to create the binary mask\n",
    "    _, mask = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Use morphological opening to remove small noise artifacts from the mask\n",
    "    kernel_morph = np.ones((3, 3), np.uint8)\n",
    "    mask_clean = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_morph, iterations=1)\n",
    "\n",
    "    return mask_clean"
   ],
   "id": "d901f2579f2574d8",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:52:17.079869Z",
     "start_time": "2025-02-23T06:52:17.075001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_image_and_mask(image_path):\n",
    "    \"\"\"Preprocess an image and create its segmentation mask.\"\"\"\n",
    "    # Read and preprocess image\n",
    "    image = cv2.imread(str(image_path))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create mask\n",
    "    try:\n",
    "        mask = create_mask_otsu(image)\n",
    "        mask = (mask > 0).astype(np.uint8)  # Convert to binary 0/1\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Mask creation failed for {image_path}. Using fallback mask.\")\n",
    "        mask = np.ones(image.shape[:2], dtype=np.uint8)  # Fallback: use entire image\n",
    "\n",
    "    # Resize both image and mask to 224x224\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    mask = cv2.resize(mask, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Normalize image to [0,1]\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "\n",
    "    return image, mask"
   ],
   "id": "4098378f3e62e7c1",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:52:19.237076Z",
     "start_time": "2025-02-23T06:52:19.232942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_list_feature(value):\n",
    "    \"\"\"Returns a float_list from a numpy array.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value.flatten()))\n",
    "\n",
    "def serialize_example(image, mask, metadata, label):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Convert image to bytes\n",
    "    image_bytes = tf.io.encode_jpeg(tf.cast(image * 255, tf.uint8)).numpy()\n",
    "\n",
    "    # Convert mask to bytes - ensure mask is 3D\n",
    "    mask_3d = np.expand_dims(mask, axis=-1)  # Add channel dimension\n",
    "    mask_bytes = tf.io.encode_jpeg(tf.cast(mask_3d * 255, tf.uint8)).numpy()\n",
    "\n",
    "    feature = {\n",
    "        'image': _bytes_feature(image_bytes),\n",
    "        'mask': _bytes_feature(mask_bytes),\n",
    "        'metadata': tf.train.Feature(float_list=tf.train.FloatList(value=metadata)),\n",
    "        'label': tf.train.Feature(float_list=tf.train.FloatList(value=[label]))\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ],
   "id": "59a8111b3259942c",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:52:21.717799Z",
     "start_time": "2025-02-23T06:52:21.712940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def write_tfrecord(data, filename):\n",
    "    \"\"\"Write dataset to TFRecord including segmentation masks.\"\"\"\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for idx, row in data.iterrows():\n",
    "            try:\n",
    "                # Process image and create mask\n",
    "                image, mask = preprocess_image_and_mask(row['image_path'])\n",
    "\n",
    "                # Get metadata and label\n",
    "                metadata_cols = [col for col in data.columns if col not in ['isic_id', 'image_path', 'label']]\n",
    "                metadata = row[metadata_cols].values.astype(np.float32)\n",
    "                label = row['label']\n",
    "\n",
    "                # Create and write TF Example\n",
    "                tf_example = serialize_example(image, mask, metadata, label)\n",
    "                writer.write(tf_example.SerializeToString())\n",
    "\n",
    "                if idx % 100 == 0:\n",
    "                    print(f\"Processed {idx} images\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {row['image_path']}: {str(e)}\")\n",
    "                continue"
   ],
   "id": "f9c6a85db78bb758",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:52:24.487655Z",
     "start_time": "2025-02-23T06:52:24.482673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "    \"\"\"Parse TFRecord dataset.\"\"\"\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'mask': tf.io.FixedLenFeature([], tf.string),\n",
    "        'metadata': tf.io.VarLenFeature(tf.float32),\n",
    "        'label': tf.io.FixedLenFeature([1], tf.float32)\n",
    "    }\n",
    "\n",
    "    features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    # Decode image and mask\n",
    "    image = tf.io.decode_jpeg(features['image'], channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    mask = tf.io.decode_jpeg(features['mask'], channels=1)\n",
    "    mask = tf.squeeze(mask)  # Remove the channel dimension when reading\n",
    "    mask = tf.cast(mask, tf.float32) / 255.0\n",
    "\n",
    "    # Handle metadata\n",
    "    metadata = tf.sparse.to_dense(features['metadata'])\n",
    "\n",
    "    return (image, mask, metadata), features['label']"
   ],
   "id": "57e4f6e904d11246",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-23T06:52:26.911871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "write_tfrecord(train_data, r\"E:\\Capstone Skin Cancer Project\\Datasets\\train.tfrecord\")\n",
    "write_tfrecord(test_data, r\"E:\\Capstone Skin Cancer Project\\Datasets\\test.tfrecord\")\n",
    "\n",
    "print(\"âœ… TFRecord creation complete!\")"
   ],
   "id": "dff3f108756c5caa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 180600 images\n",
      "Processed 233500 images\n",
      "Processed 171000 images\n",
      "Processed 105500 images\n",
      "Processed 96400 images\n",
      "Processed 26600 images\n",
      "Processed 39600 images\n",
      "Processed 189600 images\n",
      "Processed 131200 images\n",
      "Processed 181600 images\n",
      "Processed 226300 images\n",
      "Processed 74200 images\n",
      "Processed 55300 images\n",
      "Processed 90300 images\n",
      "Processed 98500 images\n",
      "Processed 392400 images\n",
      "Processed 15700 images\n",
      "Processed 233400 images\n",
      "Processed 111100 images\n",
      "Processed 339100 images\n",
      "Processed 161000 images\n",
      "Processed 221700 images\n",
      "Processed 15900 images\n",
      "Processed 399600 images\n",
      "Processed 345400 images\n",
      "Processed 273600 images\n",
      "Processed 118100 images\n",
      "Processed 340500 images\n",
      "Processed 14900 images\n",
      "Processed 290100 images\n",
      "Processed 104200 images\n",
      "Processed 233300 images\n",
      "Processed 70100 images\n",
      "Processed 368500 images\n",
      "Processed 163200 images\n",
      "Processed 385500 images\n",
      "Processed 265100 images\n",
      "Processed 213600 images\n",
      "Processed 228900 images\n",
      "Processed 287700 images\n",
      "Processed 83700 images\n",
      "Processed 339500 images\n",
      "Processed 186600 images\n",
      "Processed 313000 images\n",
      "Processed 390700 images\n",
      "Processed 189700 images\n",
      "Processed 333100 images\n",
      "Processed 359900 images\n",
      "Processed 337500 images\n",
      "Processed 213900 images\n",
      "Processed 285600 images\n",
      "Processed 283400 images\n",
      "Processed 414400 images\n",
      "Processed 315600 images\n",
      "Processed 254100 images\n",
      "Processed 124700 images\n",
      "Processed 27800 images\n",
      "Processed 67900 images\n",
      "Processed 245700 images\n",
      "Processed 303200 images\n",
      "Processed 367700 images\n",
      "Processed 375800 images\n",
      "Processed 343400 images\n",
      "Processed 22900 images\n",
      "Processed 173900 images\n",
      "Processed 263900 images\n",
      "Processed 351100 images\n",
      "Processed 312000 images\n",
      "Processed 335800 images\n",
      "Processed 16100 images\n",
      "Processed 221500 images\n",
      "Processed 67800 images\n",
      "Processed 149900 images\n",
      "Processed 110600 images\n",
      "Processed 115400 images\n",
      "Processed 376200 images\n",
      "Processed 111000 images\n",
      "Processed 247800 images\n",
      "Processed 357400 images\n",
      "Processed 3200 images\n",
      "Processed 168400 images\n",
      "Processed 269400 images\n",
      "Processed 349300 images\n",
      "Processed 242400 images\n",
      "Processed 406600 images\n",
      "Processed 294900 images\n",
      "Processed 65500 images\n",
      "Processed 34300 images\n",
      "Processed 292300 images\n",
      "Processed 342200 images\n",
      "Processed 133100 images\n",
      "Processed 266500 images\n",
      "Processed 159100 images\n",
      "Processed 271100 images\n",
      "Processed 195300 images\n",
      "Processed 82300 images\n",
      "Processed 216400 images\n",
      "Processed 290500 images\n",
      "Processed 68100 images\n",
      "Processed 329800 images\n",
      "Processed 107400 images\n",
      "Processed 72300 images\n",
      "Processed 263800 images\n",
      "Processed 412800 images\n",
      "Processed 7900 images\n",
      "Processed 178200 images\n",
      "Processed 189500 images\n",
      "Processed 41600 images\n",
      "Processed 182300 images\n",
      "Processed 266800 images\n",
      "Processed 379000 images\n",
      "Processed 209700 images\n",
      "Processed 187000 images\n",
      "Processed 5000 images\n",
      "Processed 210800 images\n",
      "Processed 259400 images\n",
      "Processed 251500 images\n",
      "Processed 57800 images\n",
      "Processed 139000 images\n",
      "Processed 148800 images\n",
      "Processed 264700 images\n",
      "Processed 394700 images\n",
      "Processed 173400 images\n",
      "Processed 93000 images\n",
      "Processed 258100 images\n",
      "Processed 356800 images\n",
      "Processed 37000 images\n",
      "Processed 330800 images\n",
      "Processed 38400 images\n",
      "Processed 277800 images\n",
      "Processed 414700 images\n",
      "Processed 83200 images\n",
      "Processed 95700 images\n",
      "Processed 336500 images\n",
      "Processed 346100 images\n",
      "Processed 312600 images\n",
      "Processed 380000 images\n",
      "Processed 153800 images\n",
      "Processed 6700 images\n",
      "Processed 230000 images\n",
      "Processed 402300 images\n",
      "Processed 187900 images\n",
      "Processed 331200 images\n",
      "Processed 217600 images\n",
      "Processed 129600 images\n",
      "Processed 222000 images\n",
      "Processed 17600 images\n",
      "Processed 388200 images\n",
      "Processed 293600 images\n",
      "Processed 41300 images\n",
      "Processed 290300 images\n",
      "Processed 103400 images\n",
      "Processed 307000 images\n",
      "Processed 134600 images\n",
      "Processed 235000 images\n",
      "Processed 197200 images\n",
      "Processed 216200 images\n",
      "Processed 23700 images\n",
      "Processed 23900 images\n",
      "Processed 187600 images\n",
      "Processed 220300 images\n",
      "Processed 104100 images\n",
      "Processed 44600 images\n",
      "Processed 253900 images\n",
      "Processed 165000 images\n",
      "Processed 168100 images\n",
      "Processed 408900 images\n",
      "Processed 76800 images\n",
      "Processed 236900 images\n",
      "Processed 203800 images\n",
      "Processed 410900 images\n",
      "Processed 24400 images\n",
      "Processed 361900 images\n",
      "Processed 47900 images\n",
      "Processed 201400 images\n",
      "Processed 169700 images\n",
      "Processed 295000 images\n",
      "Processed 131400 images\n",
      "Processed 410500 images\n",
      "Processed 167600 images\n",
      "Processed 5400 images\n",
      "Processed 62600 images\n",
      "Processed 258600 images\n",
      "Processed 202800 images\n",
      "Processed 375400 images\n",
      "Processed 109400 images\n",
      "Processed 365700 images\n",
      "Processed 165400 images\n",
      "Processed 313300 images\n",
      "Processed 103800 images\n",
      "Processed 58600 images\n",
      "Processed 296900 images\n",
      "Processed 43900 images\n",
      "Processed 101500 images\n",
      "Processed 161700 images\n",
      "Processed 64900 images\n",
      "Processed 355000 images\n",
      "Processed 256100 images\n",
      "Processed 373400 images\n",
      "Processed 229800 images\n",
      "Processed 15000 images\n",
      "Processed 136100 images\n",
      "Processed 392600 images\n",
      "Processed 160300 images\n",
      "Processed 353800 images\n",
      "Processed 172700 images\n",
      "Processed 48300 images\n",
      "Processed 123000 images\n",
      "Processed 54000 images\n",
      "Processed 17500 images\n",
      "Processed 140900 images\n",
      "Processed 241800 images\n",
      "Processed 76000 images\n",
      "Processed 285100 images\n",
      "Processed 325400 images\n",
      "Processed 205800 images\n",
      "Processed 264600 images\n",
      "Processed 186400 images\n",
      "Processed 199200 images\n",
      "Processed 35100 images\n",
      "Processed 271000 images\n",
      "Processed 360700 images\n",
      "Processed 126500 images\n",
      "Processed 95800 images\n",
      "Processed 322600 images\n",
      "Processed 343700 images\n",
      "Processed 195400 images\n",
      "Processed 332500 images\n",
      "Processed 181700 images\n",
      "Processed 42800 images\n",
      "Processed 92900 images\n",
      "Processed 19200 images\n",
      "Processed 270800 images\n",
      "Processed 36000 images\n",
      "Processed 364500 images\n",
      "Processed 204800 images\n",
      "Processed 29700 images\n",
      "Processed 142100 images\n",
      "Processed 192900 images\n",
      "Processed 328400 images\n",
      "Processed 341900 images\n",
      "Processed 241500 images\n",
      "Processed 89800 images\n",
      "Processed 224900 images\n",
      "Processed 3500 images\n",
      "Processed 308800 images\n",
      "Processed 31600 images\n",
      "Processed 260600 images\n",
      "Processed 412200 images\n",
      "Processed 89700 images\n",
      "Processed 391400 images\n",
      "Processed 188300 images\n",
      "Processed 336000 images\n",
      "Processed 81300 images\n",
      "Processed 183900 images\n",
      "Processed 67600 images\n",
      "Processed 135400 images\n",
      "Processed 104600 images\n",
      "Processed 327800 images\n",
      "Processed 43500 images\n",
      "Processed 233000 images\n",
      "Processed 125500 images\n",
      "Processed 360500 images\n",
      "Processed 340900 images\n",
      "Processed 395500 images\n",
      "Processed 170200 images\n",
      "Processed 127800 images\n",
      "Processed 2800 images\n",
      "Processed 23800 images\n",
      "Processed 369000 images\n",
      "Processed 234500 images\n",
      "Processed 239900 images\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example of how to read the data back\n",
    "dataset = tf.data.TFRecordDataset(r\"E:\\Capstone Skin Cancer Project\\Datasets\\train.tfrecord\")\n",
    "parsed_dataset = dataset.map(parse_tfrecord)"
   ],
   "id": "956cb924cbb62ad7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
